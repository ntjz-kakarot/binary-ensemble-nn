{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bda5d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "0.16.0\n",
      "0.9.1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import brevitas\n",
    "from brevitas.nn import QuantLinear\n",
    "from brevitas.core.quant.binary import ClampedBinaryQuant\n",
    "from brevitas.core.scaling import ConstScaling\n",
    "from brevitas.core.quant import QuantType\n",
    "print(brevitas.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af5ff2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb38195",
   "metadata": {},
   "source": [
    "# Network Architectures\n",
    "\n",
    "## Simple BNN vs. Real-Valued DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb46fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-valued DNN with a single layer\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 10)  # For CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32 * 32 * 3)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BNN, self).__init__()\n",
    "        \n",
    "        # Use predefined BINARY weight quant type\n",
    "        self.fc1 = QuantLinear(\n",
    "            32 * 32 * 3, \n",
    "            10, \n",
    "            bias=True, \n",
    "            weight_quant_type=QuantType.BINARY)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32 * 32 * 3)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344f2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = DNN()\n",
    "bnn_model = BNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197dc458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            30,730\n",
      "=================================================================\n",
      "Total params: 30,730\n",
      "Trainable params: 30,730\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "30730\n",
      "================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "├─QuantLinear: 1-1                                      --\n",
      "|    └─ActQuantProxyFromInjector: 2-1                   --\n",
      "|    |    └─StatelessBuffer: 3-1                        --\n",
      "|    └─ActQuantProxyFromInjector: 2-2                   --\n",
      "|    |    └─StatelessBuffer: 3-2                        --\n",
      "|    └─WeightQuantProxyFromInjector: 2-3                --\n",
      "|    |    └─StatelessBuffer: 3-3                        --\n",
      "|    |    └─BinaryQuant: 3-4                            30,720\n",
      "|    └─BiasQuantProxyFromInjector: 2-4                  --\n",
      "|    |    └─StatelessBuffer: 3-5                        --\n",
      "================================================================================\n",
      "Total params: 30,720\n",
      "Trainable params: 30,720\n",
      "Non-trainable params: 0\n",
      "================================================================================\n",
      "30720\n"
     ]
    }
   ],
   "source": [
    "print(summary(dnn_model).trainable_params)\n",
    "print(summary(bnn_model).trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07702b6",
   "metadata": {},
   "source": [
    "## XNOR NIN vs. Real-Valued NIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> class Exp(Function):\n",
    ">>>     @staticmethod\n",
    ">>>     def forward(ctx, i):\n",
    ">>>         result = i.exp()\n",
    ">>>         ctx.save_for_backward(result)\n",
    ">>>         return result\n",
    ">>>\n",
    ">>>     @staticmethod\n",
    ">>>     def backward(ctx, grad_output):\n",
    ">>>         result, = ctx.saved_tensors\n",
    ">>>         return grad_output * result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01b3105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BinActive(torch.autograd.Function):\n",
    "    '''\n",
    "    Binarize the input activations and calculate the mean across channel dimension.\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        size = input.size()\n",
    "        mean = torch.mean(input.abs(), 1, keepdim=True)\n",
    "        input = input.sign()\n",
    "        return input, mean\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, grad_output_mean):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input.ge(1)] = 0\n",
    "        grad_input[input.le(-1)] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class BinConv2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels,\n",
    "            kernel_size=-1, stride=-1, padding=-1, dropout=0):\n",
    "        super(BinConv2d, self).__init__()\n",
    "        self.layer_type = 'BinConv2d'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dropout_ratio = dropout\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n",
    "        self.bn.weight.data = self.bn.weight.data.zero_().add(1.0)\n",
    "        if dropout!=0:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels,\n",
    "                kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x, mean = BinActive.apply(x)\n",
    "        if self.dropout_ratio!=0:\n",
    "            x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# XNOR NIN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.xnor = nn.Sequential(\n",
    "                nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                BinConv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d( 96, 192, kernel_size=5, stride=1, padding=2, dropout=0.5),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d(192, 192, kernel_size=3, stride=1, padding=1, dropout=0.5),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                if hasattr(m.weight, 'data'):\n",
    "                    m.weight.data.clamp_(min=0.01)\n",
    "        x = self.xnor(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n",
    "    \n",
    "# Real NIN\n",
    "class RealNIN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealNIN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                nn.Conv2d(96, 192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AdaptiveAvgPool2d(1)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3959784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nin = Net()\n",
    "model_real_nin = RealNIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "718373be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       14,592\n",
      "|    └─BatchNorm2d: 2-2                  --\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─BinConv2d: 2-4                    --\n",
      "|    |    └─BatchNorm2d: 3-1             384\n",
      "|    |    └─Conv2d: 3-2                  30,880\n",
      "|    |    └─ReLU: 3-3                    --\n",
      "|    └─BinConv2d: 2-5                    --\n",
      "|    |    └─BatchNorm2d: 3-4             320\n",
      "|    |    └─Conv2d: 3-5                  15,456\n",
      "|    |    └─ReLU: 3-6                    --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "|    └─BinConv2d: 2-7                    --\n",
      "|    |    └─BatchNorm2d: 3-7             192\n",
      "|    |    └─Dropout: 3-8                 --\n",
      "|    |    └─Conv2d: 3-9                  460,992\n",
      "|    |    └─ReLU: 3-10                   --\n",
      "|    └─BinConv2d: 2-8                    --\n",
      "|    |    └─BatchNorm2d: 3-11            384\n",
      "|    |    └─Conv2d: 3-12                 37,056\n",
      "|    |    └─ReLU: 3-13                   --\n",
      "|    └─BinConv2d: 2-9                    --\n",
      "|    |    └─BatchNorm2d: 3-14            384\n",
      "|    |    └─Conv2d: 3-15                 37,056\n",
      "|    |    └─ReLU: 3-16                   --\n",
      "|    └─AvgPool2d: 2-10                   --\n",
      "|    └─BinConv2d: 2-11                   --\n",
      "|    |    └─BatchNorm2d: 3-17            384\n",
      "|    |    └─Dropout: 3-18                --\n",
      "|    |    └─Conv2d: 3-19                 331,968\n",
      "|    |    └─ReLU: 3-20                   --\n",
      "|    └─BinConv2d: 2-12                   --\n",
      "|    |    └─BatchNorm2d: 3-21            384\n",
      "|    |    └─Conv2d: 3-22                 37,056\n",
      "|    |    └─ReLU: 3-23                   --\n",
      "|    └─BatchNorm2d: 2-13                 --\n",
      "|    └─Conv2d: 2-14                      1,930\n",
      "|    └─ReLU: 2-15                        --\n",
      "|    └─AvgPool2d: 2-16                   --\n",
      "=================================================================\n",
      "Total params: 969,418\n",
      "Trainable params: 969,418\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "969418\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       14,592\n",
      "|    └─BatchNorm2d: 2-2                  384\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─Conv2d: 2-4                       30,880\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─Conv2d: 2-6                       15,456\n",
      "|    └─ReLU: 2-7                         --\n",
      "|    └─MaxPool2d: 2-8                    --\n",
      "|    └─Conv2d: 2-9                       460,992\n",
      "|    └─BatchNorm2d: 2-10                 384\n",
      "|    └─ReLU: 2-11                        --\n",
      "|    └─Conv2d: 2-12                      37,056\n",
      "|    └─ReLU: 2-13                        --\n",
      "|    └─Conv2d: 2-14                      37,056\n",
      "|    └─ReLU: 2-15                        --\n",
      "|    └─AvgPool2d: 2-16                   --\n",
      "|    └─Conv2d: 2-17                      331,968\n",
      "|    └─BatchNorm2d: 2-18                 384\n",
      "|    └─ReLU: 2-19                        --\n",
      "|    └─Conv2d: 2-20                      37,056\n",
      "|    └─ReLU: 2-21                        --\n",
      "|    └─Conv2d: 2-22                      1,930\n",
      "|    └─ReLU: 2-23                        --\n",
      "|    └─AdaptiveAvgPool2d: 2-24           --\n",
      "=================================================================\n",
      "Total params: 968,138\n",
      "Trainable params: 968,138\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "968138\n"
     ]
    }
   ],
   "source": [
    "print(summary(model_nin).trainable_params)\n",
    "print(summary(model_real_nin).trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4be05",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "## BNN vs. RDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9130f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.154208042705059\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(dnn_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m train(dnn_model, trainloader, criterion, optimizer)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the BNN\u001b[39;00m\n\u001b[1;32m     21\u001b[0m optimizer_bnn \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(bnn_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     11\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bnn/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bnn/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/bnn/lib/python3.11/site-packages/torch/optim/sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     71\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 75\u001b[0m sgd(params_with_grad,\n\u001b[1;32m     76\u001b[0m     d_p_list,\n\u001b[1;32m     77\u001b[0m     momentum_buffer_list,\n\u001b[1;32m     78\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     79\u001b[0m     momentum\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     80\u001b[0m     lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     81\u001b[0m     dampening\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdampening\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     82\u001b[0m     nesterov\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnesterov\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     83\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     84\u001b[0m     has_sparse_grad\u001b[38;5;241m=\u001b[39mhas_sparse_grad,\n\u001b[1;32m     85\u001b[0m     foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/miniconda3/envs/bnn/lib/python3.11/site-packages/torch/optim/sgd.py:208\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# why must we be explicit about an if statement for torch.jit.is_scripting here?\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# because JIT can't handle Optionals nor fancy conditionals when scripting\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m--> 208\u001b[0m         _, foreach \u001b[38;5;241m=\u001b[39m _default_to_fused_or_foreach(params, differentiable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_fused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         foreach \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bnn/lib/python3.11/site-packages/torch/optim/optimizer.py:116\u001b[0m, in \u001b[0;36m_default_to_fused_or_foreach\u001b[0;34m(params, differentiable, use_fused)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    115\u001b[0m fused_supported_devices \u001b[38;5;241m=\u001b[39m _get_fused_kernels_supported_devices()\n\u001b[0;32m--> 116\u001b[0m foreach_supported_devices \u001b[38;5;241m=\u001b[39m _get_foreach_kernels_supported_devices()\n\u001b[1;32m    117\u001b[0m fused \u001b[38;5;241m=\u001b[39m use_fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    118\u001b[0m     p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(p) \u001b[38;5;129;01min\u001b[39;00m _foreach_supported_types \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    119\u001b[0m                   p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m fused_supported_devices \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    120\u001b[0m                   torch\u001b[38;5;241m.\u001b[39mis_floating_point(p)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[1;32m    121\u001b[0m )\n\u001b[1;32m    122\u001b[0m foreach \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    123\u001b[0m     p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(p) \u001b[38;5;129;01min\u001b[39;00m _foreach_supported_types \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    124\u001b[0m                   p\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01min\u001b[39;00m foreach_supported_devices) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[1;32m    125\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bnn/lib/python3.11/site-packages/torch/utils/_foreach_utils.py:12\u001b[0m, in \u001b[0;36m_get_foreach_kernels_supported_devices\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_foreach_kernels_supported_devices\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Return the device type list that supports foreach kernels.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader)}\")\n",
    "    print('Finished Training')\n",
    "\n",
    "# Train the real-valued DNN\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(dnn_model.parameters(), lr=0.001, momentum=0.9)\n",
    "train(dnn_model, trainloader, criterion, optimizer)\n",
    "\n",
    "# Train the BNN\n",
    "optimizer_bnn = optim.SGD(bnn_model.parameters(), lr=0.001, momentum=0.9)\n",
    "train(bnn_model, trainloader, criterion, optimizer_bnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba1d52",
   "metadata": {},
   "source": [
    "## NIN vs. Real NIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4847e4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7395923990535735\n",
      "Epoch 2, Loss: 1.372889082570076\n",
      "Epoch 3, Loss: 1.1988270066076516\n",
      "Epoch 4, Loss: 1.0995428943142296\n",
      "Epoch 5, Loss: 1.027989880823195\n",
      "Epoch 6, Loss: 0.9693295143486559\n",
      "Epoch 7, Loss: 0.9244582037266343\n",
      "Epoch 8, Loss: 0.8879605935487896\n",
      "Epoch 9, Loss: 0.8537723453085124\n",
      "Epoch 10, Loss: 0.8277965047960728\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader)}\")\n",
    "    print('Finished Training')\n",
    "\n",
    "# # Train the real-valued DNN\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model_real_nin.parameters(), lr=0.001, momentum=0.9)\n",
    "# train(model_real_nin, trainloader, criterion, optimizer)\n",
    "\n",
    "# Train the BNN\n",
    "optimizer_bnn = optim.SGD(model_nin.parameters(), lr=0.001, momentum=0.9)\n",
    "train(model_nin, trainloader, criterion, optimizer_bnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "dnn_accuracy = evaluate(dnn_model, testloader)\n",
    "bnn_accuracy = evaluate(bnn_model, testloader)\n",
    "\n",
    "print(f'DNN Accuracy on test set: {dnn_accuracy}%')\n",
    "print(f'BNN Accuracy on test set: {bnn_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bae0f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNN Accuracy on test set: 70.05%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# dnn_accuracy = evaluate(dnn_model, testloader)\n",
    "nin_accuracy = evaluate(model_nin, testloader)\n",
    "\n",
    "# print(f'DNN Accuracy on test set: {dnn_accuracy}%')\n",
    "print(f'BNN Accuracy on test set: {nin_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5f2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
