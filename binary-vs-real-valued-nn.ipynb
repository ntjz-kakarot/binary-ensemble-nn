{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bda5d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "0.16.0\n",
      "0.9.1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import brevitas\n",
    "from brevitas.nn import QuantLinear\n",
    "from brevitas.core.quant.binary import ClampedBinaryQuant\n",
    "from brevitas.core.scaling import ConstScaling\n",
    "from brevitas.core.quant import QuantType\n",
    "print(brevitas.__version__)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5ff2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb38195",
   "metadata": {},
   "source": [
    "# Network Architectures\n",
    "\n",
    "## Simple BNN vs. Real-Valued DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb46fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-valued DNN with a single layer\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 10)  # For CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32 * 32 * 3)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BNN, self).__init__()\n",
    "        \n",
    "        # Use predefined BINARY weight quant type\n",
    "        self.fc1 = QuantLinear(\n",
    "            32 * 32 * 3, \n",
    "            10, \n",
    "            bias=True, \n",
    "            weight_quant_type=QuantType.BINARY)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32 * 32 * 3)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "344f2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = DNN()\n",
    "bnn_model = BNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "197dc458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            30,730\n",
      "=================================================================\n",
      "Total params: 30,730\n",
      "Trainable params: 30,730\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "30730\n",
      "================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "├─QuantLinear: 1-1                                      --\n",
      "|    └─ActQuantProxyFromInjector: 2-1                   --\n",
      "|    |    └─StatelessBuffer: 3-1                        --\n",
      "|    └─ActQuantProxyFromInjector: 2-2                   --\n",
      "|    |    └─StatelessBuffer: 3-2                        --\n",
      "|    └─WeightQuantProxyFromInjector: 2-3                --\n",
      "|    |    └─StatelessBuffer: 3-3                        --\n",
      "|    |    └─BinaryQuant: 3-4                            30,720\n",
      "|    └─BiasQuantProxyFromInjector: 2-4                  --\n",
      "|    |    └─StatelessBuffer: 3-5                        --\n",
      "================================================================================\n",
      "Total params: 30,720\n",
      "Trainable params: 30,720\n",
      "Non-trainable params: 0\n",
      "================================================================================\n",
      "30720\n"
     ]
    }
   ],
   "source": [
    "print(summary(dnn_model).trainable_params)\n",
    "print(summary(bnn_model).trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07702b6",
   "metadata": {},
   "source": [
    "## XNOR NIN vs. Real-Valued NIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01b3105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BinActive(torch.autograd.Function):\n",
    "    '''\n",
    "    Binarize the input activations and calculate the mean across channel dimension.\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        size = input.size()\n",
    "        mean = torch.mean(input.abs(), 1, keepdim=True)\n",
    "        input = input.sign()\n",
    "        return input, mean\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, grad_output_mean):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input.ge(1)] = 0\n",
    "        grad_input[input.le(-1)] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class BinConv2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels,\n",
    "            kernel_size=-1, stride=-1, padding=-1, dropout=0):\n",
    "        super(BinConv2d, self).__init__()\n",
    "        self.layer_type = 'BinConv2d'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dropout_ratio = dropout\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n",
    "        self.bn.weight.data = self.bn.weight.data.zero_().add(1.0)\n",
    "        if dropout!=0:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels,\n",
    "                kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x, mean = BinActive.apply(x)\n",
    "        if self.dropout_ratio!=0:\n",
    "            x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# XNOR NIN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.xnor = nn.Sequential(\n",
    "                nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                BinConv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d( 96, 192, kernel_size=5, stride=1, padding=2, dropout=0.5),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d(192, 192, kernel_size=3, stride=1, padding=1, dropout=0.5),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                if hasattr(m.weight, 'data'):\n",
    "                    m.weight.data.clamp_(min=0.01)\n",
    "        x = self.xnor(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n",
    "    \n",
    "# Real NIN\n",
    "class RealNIN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealNIN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                nn.Conv2d(96, 192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AdaptiveAvgPool2d(1)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e389598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimerHook:\n",
    "    def __init__(self):\n",
    "        self.start_time = 0\n",
    "        self.total_time = 0\n",
    "\n",
    "    def __call__(self, module, input, output):\n",
    "        if self.start_time == 0:\n",
    "            self.start_time = time.time()\n",
    "        else:\n",
    "            end_time = time.time()\n",
    "            self.total_time += end_time - self.start_time\n",
    "            self.start_time = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.start_time = 0\n",
    "        self.total_time = 0\n",
    "\n",
    "    def total(self):\n",
    "        return self.total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3959784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nin = Net()\n",
    "# model_real_nin = RealNIN()\n",
    "\n",
    "hooks = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "718373be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       14,592\n",
      "|    └─BatchNorm2d: 2-2                  --\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─BinConv2d: 2-4                    --\n",
      "|    |    └─BatchNorm2d: 3-1             384\n",
      "|    |    └─Conv2d: 3-2                  30,880\n",
      "|    |    └─ReLU: 3-3                    --\n",
      "|    └─BinConv2d: 2-5                    --\n",
      "|    |    └─BatchNorm2d: 3-4             320\n",
      "|    |    └─Conv2d: 3-5                  15,456\n",
      "|    |    └─ReLU: 3-6                    --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "|    └─BinConv2d: 2-7                    --\n",
      "|    |    └─BatchNorm2d: 3-7             192\n",
      "|    |    └─Dropout: 3-8                 --\n",
      "|    |    └─Conv2d: 3-9                  460,992\n",
      "|    |    └─ReLU: 3-10                   --\n",
      "|    └─BinConv2d: 2-8                    --\n",
      "|    |    └─BatchNorm2d: 3-11            384\n",
      "|    |    └─Conv2d: 3-12                 37,056\n",
      "|    |    └─ReLU: 3-13                   --\n",
      "|    └─BinConv2d: 2-9                    --\n",
      "|    |    └─BatchNorm2d: 3-14            384\n",
      "|    |    └─Conv2d: 3-15                 37,056\n",
      "|    |    └─ReLU: 3-16                   --\n",
      "|    └─AvgPool2d: 2-10                   --\n",
      "|    └─BinConv2d: 2-11                   --\n",
      "|    |    └─BatchNorm2d: 3-17            384\n",
      "|    |    └─Dropout: 3-18                --\n",
      "|    |    └─Conv2d: 3-19                 331,968\n",
      "|    |    └─ReLU: 3-20                   --\n",
      "|    └─BinConv2d: 2-12                   --\n",
      "|    |    └─BatchNorm2d: 3-21            384\n",
      "|    |    └─Conv2d: 3-22                 37,056\n",
      "|    |    └─ReLU: 3-23                   --\n",
      "|    └─BatchNorm2d: 2-13                 --\n",
      "|    └─Conv2d: 2-14                      1,930\n",
      "|    └─ReLU: 2-15                        --\n",
      "|    └─AvgPool2d: 2-16                   --\n",
      "=================================================================\n",
      "Total params: 969,418\n",
      "Trainable params: 969,418\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "969418\n"
     ]
    }
   ],
   "source": [
    "print(summary(model_nin).trainable_params)\n",
    "# print(summary(model_real_nin).trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6d65b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use named_children to get immediate child modules. If you need all nested modules, use named_modules instead.\n",
    "for name, layer in model_nin.named_children():\n",
    "    if isinstance(layer, nn.Sequential):  # since 'net' is Sequential\n",
    "        for sub_name, sub_layer in layer.named_children():\n",
    "            hook = TimerHook()\n",
    "            sub_layer.register_forward_hook(hook)\n",
    "            hooks[sub_name] = hook\n",
    "    else:\n",
    "        hook = TimerHook()\n",
    "        layer.register_forward_hook(hook)\n",
    "        hooks[name] = hook\n",
    "\n",
    "# After an epoch, print the aggregated times\n",
    "def print_times_and_reset():\n",
    "    for name, hook in hooks.items():\n",
    "        print(f\"Total time for {name}: {hook.total()} seconds\")\n",
    "        hook.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4be05",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "## BNN vs. RDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9130f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.155623477058411\n",
      "Epoch 2, Loss: 2.1059559034729003\n",
      "Epoch 3, Loss: 2.0895450543379783\n",
      "Epoch 4, Loss: 2.0689636600238086\n",
      "Epoch 5, Loss: 2.0588250145411493\n",
      "Epoch 6, Loss: 2.0522228903388977\n",
      "Epoch 7, Loss: 2.0453778921216728\n",
      "Epoch 8, Loss: 2.0422889260053636\n",
      "Epoch 9, Loss: 2.036768742130399\n",
      "Epoch 10, Loss: 2.0323982599473\n",
      "Finished Training\n",
      "Epoch 1, Loss: 2.1664529304897786\n",
      "Epoch 2, Loss: 2.1771481917607782\n",
      "Epoch 3, Loss: 2.191689472646117\n",
      "Epoch 4, Loss: 2.2208557810384035\n",
      "Epoch 5, Loss: 2.2242107924509047\n",
      "Epoch 6, Loss: 2.202486847819686\n",
      "Epoch 7, Loss: 2.240039607576132\n",
      "Epoch 8, Loss: 2.252634796155095\n",
      "Epoch 9, Loss: 2.2478501925316454\n",
      "Epoch 10, Loss: 2.249462173079848\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader)}\")\n",
    "    print('Finished Training')\n",
    "\n",
    "# Train the real-valued DNN\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(dnn_model.parameters(), lr=0.001, momentum=0.9)\n",
    "train(dnn_model, trainloader, criterion, optimizer)\n",
    "\n",
    "# Train the BNN\n",
    "optimizer_bnn = optim.SGD(bnn_model.parameters(), lr=0.001, momentum=0.9)\n",
    "train(bnn_model, trainloader, criterion, optimizer_bnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba1d52",
   "metadata": {},
   "source": [
    "## NIN vs. Real NIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4847e4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7454025261569024\n",
      "Total time for 0: 165.91272497177124 seconds\n",
      "Total time for 1: 165.91626811027527 seconds\n",
      "Total time for 2: 165.9168107509613 seconds\n",
      "Total time for 3: 165.90263390541077 seconds\n",
      "Total time for 4: 165.9103901386261 seconds\n",
      "Total time for 5: 165.90477681159973 seconds\n",
      "Total time for 6: 165.91030263900757 seconds\n",
      "Total time for 7: 165.93570709228516 seconds\n",
      "Total time for 8: 165.9660141468048 seconds\n",
      "Total time for 9: 165.97250533103943 seconds\n",
      "Total time for 10: 165.97922015190125 seconds\n",
      "Total time for 11: 165.98755264282227 seconds\n",
      "Total time for 12: 165.99263215065002 seconds\n",
      "Total time for 13: 165.99957704544067 seconds\n",
      "Total time for 14: 166.00323724746704 seconds\n",
      "Total time for 15: 166.00459933280945 seconds\n",
      "Epoch 2, Loss: 1.3910341137969493\n",
      "Total time for 0: 164.85368847846985 seconds\n",
      "Total time for 1: 164.86217665672302 seconds\n",
      "Total time for 2: 164.865398645401 seconds\n",
      "Total time for 3: 164.9595353603363 seconds\n",
      "Total time for 4: 164.94984126091003 seconds\n",
      "Total time for 5: 164.9618752002716 seconds\n",
      "Total time for 6: 164.96733927726746 seconds\n",
      "Total time for 7: 164.97088479995728 seconds\n",
      "Total time for 8: 164.97244596481323 seconds\n",
      "Total time for 9: 164.97229886054993 seconds\n",
      "Total time for 10: 164.93311071395874 seconds\n",
      "Total time for 11: 164.9042525291443 seconds\n",
      "Total time for 12: 164.90428519248962 seconds\n",
      "Total time for 13: 164.90551733970642 seconds\n",
      "Total time for 14: 164.90766501426697 seconds\n",
      "Total time for 15: 164.90927028656006 seconds\n",
      "Epoch 3, Loss: 1.2265420114564896\n",
      "Total time for 0: 164.5140244960785 seconds\n",
      "Total time for 1: 164.52974224090576 seconds\n",
      "Total time for 2: 164.5330789089203 seconds\n",
      "Total time for 3: 164.63169860839844 seconds\n",
      "Total time for 4: 164.67225074768066 seconds\n",
      "Total time for 5: 164.68173480033875 seconds\n",
      "Total time for 6: 164.70915699005127 seconds\n",
      "Total time for 7: 164.71976566314697 seconds\n",
      "Total time for 8: 164.71223330497742 seconds\n",
      "Total time for 9: 164.71342706680298 seconds\n",
      "Total time for 10: 164.724924325943 seconds\n",
      "Total time for 11: 164.73250484466553 seconds\n",
      "Total time for 12: 164.73612642288208 seconds\n",
      "Total time for 13: 164.73360085487366 seconds\n",
      "Total time for 14: 164.73712396621704 seconds\n",
      "Total time for 15: 164.7365324497223 seconds\n",
      "Epoch 4, Loss: 1.1164508585816622\n",
      "Total time for 0: 167.9722180366516 seconds\n",
      "Total time for 1: 167.98512816429138 seconds\n",
      "Total time for 2: 167.98784184455872 seconds\n",
      "Total time for 3: 167.96667647361755 seconds\n",
      "Total time for 4: 167.94687294960022 seconds\n",
      "Total time for 5: 167.94465041160583 seconds\n",
      "Total time for 6: 167.9106478691101 seconds\n",
      "Total time for 7: 167.92570233345032 seconds\n",
      "Total time for 8: 167.94298315048218 seconds\n",
      "Total time for 9: 167.94390320777893 seconds\n",
      "Total time for 10: 167.97047519683838 seconds\n",
      "Total time for 11: 167.99330806732178 seconds\n",
      "Total time for 12: 168.0019087791443 seconds\n",
      "Total time for 13: 168.0132372379303 seconds\n",
      "Total time for 14: 168.01741194725037 seconds\n",
      "Total time for 15: 168.0203514099121 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# optimizer = optim.SGD(model_real_nin.parameters(), lr=0.001, momentum=0.9)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# train(model_real_nin, trainloader, criterion, optimizer)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Train the B-NIN\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer_bnn \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model_nin\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m train(model_nin, trainloader, criterion, optimizer_bnn)\n",
      "\u001b[1;32m/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb Cell 16\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nipuna/git/binary-ensemble-nn/binary-vs-real-valued-nn.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/bnn/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bnn/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bnn/lib/python3.11/site-packages/torch/autograd/function.py:276\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBackwardCFunction\u001b[39;00m(_C\u001b[39m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 276\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    277\u001b[0m         \u001b[39m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         \u001b[39m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         backward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mbackward  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         vjp_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mvjp  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader)}\")\n",
    "        print_times_and_reset()\n",
    "    print('Finished Training')\n",
    "\n",
    "# Train the real-valued NIN\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model_real_nin.parameters(), lr=0.001, momentum=0.9)\n",
    "# train(model_real_nin, trainloader, criterion, optimizer)\n",
    "\n",
    "# Train the B-NIN\n",
    "optimizer_bnn = optim.SGD(model_nin.parameters(), lr=0.001, momentum=0.9)\n",
    "train(model_nin, trainloader, criterion, optimizer_bnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae0d48",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Accuracy on test set: 33.82%\n",
      "BNN Accuracy on test set: 33.08%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "dnn_accuracy = evaluate(dnn_model, testloader)\n",
    "bnn_accuracy = evaluate(bnn_model, testloader)\n",
    "\n",
    "print(f'DNN Accuracy on test set: {dnn_accuracy}%')\n",
    "print(f'BNN Accuracy on test set: {bnn_accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-NIN Accuracy on test set: 10.0%\n",
      "B-NIN Accuracy on test set: 41.03%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "r_nin_accuracy = evaluate(model_real_nin, testloader)\n",
    "b_nin_accuracy = evaluate(model_nin, testloader)\n",
    "\n",
    "print(f'R-NIN Accuracy on test set: {r_nin_accuracy}%')\n",
    "print(f'B-NIN Accuracy on test set: {b_nin_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa5f2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
