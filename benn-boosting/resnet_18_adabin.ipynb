{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935a3f5",
   "metadata": {
    "id": "b935a3f5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Function\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a17255",
   "metadata": {
    "id": "21a17255"
   },
   "source": [
    "# Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfcb79",
   "metadata": {
    "id": "d0cfcb79"
   },
   "outputs": [],
   "source": [
    "class BinaryQuantize(Function):\n",
    "    '''\n",
    "        binary quantize function, from IR-Net\n",
    "        (https://github.com/htqin/IR-Net/blob/master/CIFAR-10/ResNet20/1w1a/modules/binaryfunction.py)\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, k, t):\n",
    "        ctx.save_for_backward(input, k, t)\n",
    "        out = torch.sign(input)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, k, t = ctx.saved_tensors\n",
    "        device = input.device\n",
    "        k, t = k.to(device), t.to(device)\n",
    "        grad_input = k * t * (1-torch.pow(torch.tanh(input * t), 2)) * grad_output\n",
    "        return grad_input, None, None\n",
    "\n",
    "class Maxout(nn.Module):\n",
    "    '''\n",
    "        Nonlinear function\n",
    "    '''\n",
    "    def __init__(self, channel, neg_init=0.25, pos_init=1.0):\n",
    "        super(Maxout, self).__init__()\n",
    "        self.neg_scale = nn.Parameter(neg_init*torch.ones(channel))\n",
    "        self.pos_scale = nn.Parameter(pos_init*torch.ones(channel))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Maxout\n",
    "        x = self.pos_scale.view(1,-1,1,1)*self.relu(x) - self.neg_scale.view(1,-1,1,1)*self.relu(-x)\n",
    "        return x\n",
    "\n",
    "class BinaryActivation(nn.Module):\n",
    "    '''\n",
    "        learnable distance and center for activation\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(BinaryActivation, self).__init__()\n",
    "        self.alpha_a = nn.Parameter(torch.tensor(1.0))\n",
    "        self.beta_a = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def gradient_approx(self, x):\n",
    "        '''\n",
    "            gradient approximation\n",
    "            (https://github.com/liuzechun/Bi-Real-net/blob/master/pytorch_implementation/BiReal18_34/birealnet.py)\n",
    "        '''\n",
    "        out_forward = torch.sign(x)\n",
    "        mask1 = x < -1\n",
    "        mask2 = x < 0\n",
    "        mask3 = x < 1\n",
    "        out1 = (-1) * mask1.type(torch.float32) + (x*x + 2*x) * (1-mask1.type(torch.float32))\n",
    "        out2 = out1 * mask2.type(torch.float32) + (-x*x + 2*x) * (1-mask2.type(torch.float32))\n",
    "        out3 = out2 * mask3.type(torch.float32) + 1 * (1- mask3.type(torch.float32))\n",
    "        out = out_forward.detach() - out3.detach() + out3\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x-self.beta_a)/self.alpha_a\n",
    "        x = self.gradient_approx(x)\n",
    "        return self.alpha_a*(x + self.beta_a)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    '''\n",
    "        for DownSample\n",
    "    '''\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "class AdaBin_Conv2d(nn.Conv2d):\n",
    "    '''\n",
    "        AdaBin Convolution\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False, a_bit=1, w_bit=1):\n",
    "        super(AdaBin_Conv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        self.a_bit = a_bit\n",
    "        self.w_bit = w_bit\n",
    "        self.k = torch.tensor([10]).float().cpu()\n",
    "        self.t = torch.tensor([0.1]).float().cpu()\n",
    "        self.binary_a = BinaryActivation()\n",
    "\n",
    "        self.filter_size = self.kernel_size[0]*self.kernel_size[1]*self.in_channels\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.a_bit==1:\n",
    "            inputs = self.binary_a(inputs)\n",
    "\n",
    "        if self.w_bit==1:\n",
    "            w = self.weight\n",
    "            beta_w = w.mean((1,2,3)).view(-1,1,1,1)\n",
    "            alpha_w = torch.sqrt(((w-beta_w)**2).sum((1,2,3))/self.filter_size).view(-1,1,1,1)\n",
    "\n",
    "            w = (w - beta_w)/alpha_w\n",
    "            wb = BinaryQuantize().apply(w, self.k, self.t)\n",
    "            weight = wb * alpha_w + beta_w\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        output = F.conv2d(inputs, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30455ba1",
   "metadata": {
    "id": "30455ba1"
   },
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8655fc",
   "metadata": {
    "id": "0b8655fc"
   },
   "outputs": [],
   "source": [
    "class BasicBlock_1w1a(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock_1w1a, self).__init__()\n",
    "        self.conv1 = AdaBin_Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.nonlinear1 = Maxout(planes)\n",
    "\n",
    "        self.conv2 = AdaBin_Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.nonlinear2 = Maxout(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                     )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(self.conv1(x))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.nonlinear1(out)\n",
    "        x1 = out\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += x1\n",
    "        out = self.nonlinear2(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.nonlinear1 = Maxout(64)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "        self.bn2 = nn.BatchNorm1d(512*block.expansion)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.nonlinear1(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.bn2(out)\n",
    "        out = self.linear(out)\n",
    "        # out = F.softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "def resnet18_1w1a():\n",
    "    return ResNet(BasicBlock_1w1a, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a_iSjxuNmCub",
   "metadata": {
    "id": "a_iSjxuNmCub"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PFHbAdyKl3O-",
   "metadata": {
    "id": "PFHbAdyKl3O-"
   },
   "outputs": [],
   "source": [
    "def plot_loss_curves(train_losses):\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def display_images(model, test_loader, use_cuda=True):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    with torch.no_grad():\n",
    "        data, target = next(iter(test_loader))  # Get a batch of data from the test_loader\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)  # Get the model's predictions\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # Get the predicted labels\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i in range(25):\n",
    "            plt.subplot(5, 5, i+1)\n",
    "            plt.imshow(data[i].cpu().squeeze().numpy(), cmap='gray')\n",
    "            plt.title(f'True: {target[i].item()}, Pred: {pred[i].item()}')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jFAedoj4l6P5",
   "metadata": {
    "id": "jFAedoj4l6P5"
   },
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ade70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "685ade70",
    "outputId": "707d527f-87dd-41a3-8acb-ef08526898ab"
   },
   "outputs": [],
   "source": [
    "def main(use_cuda=True):\n",
    "    # Check for CUDA availability and set the device accordingly\n",
    "    device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Data preparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Model initialization\n",
    "    model = resnet18_1w1a().to(device)  # Move model to the correct device\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)  # Adjust for 1-channel input\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training\n",
    "    train_losses = []\n",
    "    for epoch in range(10):  # Number of epochs\n",
    "        print(f'Starting epoch {epoch + 1}')  # Debugging: print a message at the start of each epoch\n",
    "        start_time = time.time()  # Debugging: record the start time of the epoch\n",
    "        model.train()\n",
    "        pbar = tqdm(total=len(train_loader) // 10, desc=f'Epoch {epoch + 1} Training', position=0, leave=True)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)  # Move data and target to the correct device\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            if batch_idx % 10 == 0:  # Debugging: print a message every 10 batches\n",
    "#                 print(f'Processed {batch_idx * len(data)} training examples')\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "        print(f'Finished epoch {epoch + 1} in {time.time() - start_time:.2f} seconds')  # Debugging: print the epoch duration\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)  # Move data and target to the correct device\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f'Test accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    plot_loss_curves(train_losses)\n",
    "    display_images(model, test_loader, use_cuda)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(use_cuda=True)  # Set to False to use CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zdVV1VeIgbSz",
   "metadata": {
    "id": "zdVV1VeIgbSz"
   },
   "source": [
    "# MNIST on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N0ofXapqgZqb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "N0ofXapqgZqb",
    "outputId": "b7e3b8eb-df77-4690-da34-d52fba0946c3"
   },
   "outputs": [],
   "source": [
    "def main(use_cuda=True):\n",
    "    # Check for CUDA availability and set the device accordingly\n",
    "    device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Data preparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Model initialization\n",
    "    model = resnet18_1w1a().to(device)  # Move model to the correct device\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)  # Adjust for 1-channel input\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training\n",
    "    train_losses = []\n",
    "    total_train_time = time.time()\n",
    "    for epoch in range(10):  # Number of epochs\n",
    "        print(f'Starting epoch {epoch + 1}')  # Debugging: print a message at the start of each epoch\n",
    "        start_time = time.time()  # Debugging: record the start time of the epoch\n",
    "        model.train()\n",
    "        pbar = tqdm(total=len(train_loader) // 10, desc=f'Epoch {epoch + 1} Training', position=0, leave=True)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)  # Move data and target to the correct device\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            if batch_idx % 10 == 0:  # Debugging: print a message every 10 batches\n",
    "#                 print(f'Processed {batch_idx * len(data)} training examples')\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "        # print(f'Finished epoch {epoch + 1} in {time.time() - start_time:.2f} seconds')  # Debugging: print the epoch duration\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    print(f'Finished training in {time.time() - total_train_time:.2f/60} minutes')\n",
    "\n",
    "    # Testing\n",
    "    total_test_time = time.time()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)  # Move data and target to the correct device\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f'Test accuracy: {accuracy * 100:.2f}%')\n",
    "    print(f'Finished testing in {time.time() - total_test_time:2f/60} minutes')\n",
    "\n",
    "    plot_loss_curves(train_losses)\n",
    "    display_images(model, test_loader, use_cuda)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(use_cuda=False)  # Set to False to use CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zJJ-_nxdFICE",
   "metadata": {
    "id": "zJJ-_nxdFICE"
   },
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "htBMeOZNN5Uh",
   "metadata": {
    "id": "htBMeOZNN5Uh"
   },
   "outputs": [],
   "source": [
    "def display_images(model, dataloader, use_cuda):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            break  # We only need one batch of images\n",
    "\n",
    "    # Convert the images and labels to numpy arrays\n",
    "    images = data.cpu().numpy()\n",
    "    labels = pred.cpu().numpy()\n",
    "\n",
    "    # Plot the first 25 images from the batch\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        # Transpose the image dimensions from CxHxW to HxWxC\n",
    "        plt.imshow(np.transpose(images[i], (1, 2, 0)))\n",
    "        plt.xlabel(labels[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g9ZvoXP9FK3i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g9ZvoXP9FK3i",
    "outputId": "17e34621-ac89-46f7-8859-27bd8168d303"
   },
   "outputs": [],
   "source": [
    "def main(use_cuda=True):\n",
    "    # Check for CUDA availability and set the device accordingly\n",
    "    device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Data preparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "    ])\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Model initialization\n",
    "    model = resnet18_1w1a().to(device)  # Move model to the correct device\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)  # Adjust for 3-channel input\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training\n",
    "    train_losses = []\n",
    "    total_train_time = time.time()\n",
    "    for epoch in range(10):  # Number of epochs\n",
    "        print(f'Starting epoch {epoch + 1}')  # Debugging: print a message at the start of each epoch\n",
    "        # start_time = time.time()  # Debugging: record the start time of the epoch\n",
    "        model.train()\n",
    "        pbar = tqdm(total=len(train_loader) // 10, desc=f'Epoch {epoch + 1} Training', position=0, leave=True)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)  # Move data and target to the correct device\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            if batch_idx % 10 == 0:  # Debugging: print a message every 10 batches\n",
    "#                 print(f'Processed {batch_idx * len(data)} training examples')\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "        # print(f'Finished epoch {epoch + 1} in {time.time() - start_time:.2f} seconds')  # Debugging: print the epoch duration\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    print(f'Finished training in {time.time() - total_train_time:2f} seconds')\n",
    "\n",
    "    # Testing\n",
    "    total_test_time = time.time()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)  # Move data and target to the correct device\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f'Test accuracy: {accuracy * 100:.2f}%')\n",
    "    print(f'Finished testing in {time.time() - total_test_time:2f} seconds')\n",
    "\n",
    "    plot_loss_curves(train_losses)\n",
    "    display_images(model, test_loader, use_cuda)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(use_cuda=True)  # Set to False to use CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WaXTcGuWy9ll",
   "metadata": {
    "id": "WaXTcGuWy9ll"
   },
   "source": [
    "# CIFAR-10 on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fAH_trDSy-NA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "id": "fAH_trDSy-NA",
    "outputId": "61336c5a-791a-44e0-9c0e-76b6e254ebf3"
   },
   "outputs": [],
   "source": [
    "def main(use_cuda=True):\n",
    "    # Check for CUDA availability and set the device accordingly\n",
    "    device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Data preparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to range [-1, 1]\n",
    "    ])\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Model initialization\n",
    "    model = resnet18_1w1a().to(device)  # Move model to the correct device\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)  # Adjust for 3-channel input\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training\n",
    "    train_losses = []\n",
    "    total_train_time = time.time()\n",
    "    for epoch in range(10):  # Number of epochs\n",
    "        print(f'Starting epoch {epoch + 1}')  # Debugging: print a message at the start of each epoch\n",
    "        # start_time = time.time()  # Debugging: record the start time of the epoch\n",
    "        model.train()\n",
    "        pbar = tqdm(total=len(train_loader) // 10, desc=f'Epoch {epoch + 1} Training', position=0, leave=True)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)  # Move data and target to the correct device\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            if batch_idx % 10 == 0:  # Debugging: print a message every 10 batches\n",
    "#                 print(f'Processed {batch_idx * len(data)} training examples')\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "        # print(f'Finished epoch {epoch + 1} in {time.time() - start_time:.2f} seconds')  # Debugging: print the epoch duration\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    print(f'Finished training in {time.time() - total_train_time:2f} seconds')\n",
    "\n",
    "    # Testing\n",
    "    total_test_time = time.time()\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)  # Move data and target to the correct device\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f'Test accuracy: {accuracy * 100:.2f}%')\n",
    "    print(f'Finished testing in {time.time() - total_test_time:2f} seconds')\n",
    "\n",
    "    plot_loss_curves(train_losses)\n",
    "    display_images(model, test_loader, use_cuda)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(use_cuda=False)  # Set to False to use CPU"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
