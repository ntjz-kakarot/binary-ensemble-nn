{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sto_quant(tensor):\n",
    "    # Stochastic Quantization Function\n",
    "    # Adds 1 to each element in tensor, divides by 2, adds a random value, clamps, rounds, multiplies by 2, and subtracts 1\n",
    "    return tensor.add(1.).div(2.).add(torch.rand(tensor.size()).cuda().add(-0.5)).clamp(0.,1.).round().mul(2.).add(-1.)\n",
    "\n",
    "class BinOp():\n",
    "    def __init__(self, model, mode='allbin'):\n",
    "        # Initializer for binary operations on the model\n",
    "        \n",
    "        # Count the number of Conv2d layers in the model\n",
    "        count_Conv2d = 0\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                count_Conv2d = count_Conv2d + 1\n",
    "        \n",
    "        # Ensure the mode is either 'allbin' or 'nin'\n",
    "        assert mode in ['allbin','nin'], 'No such a mode!'\n",
    "        \n",
    "        # Set the range of layers to be binarized based on the mode\n",
    "        if mode == 'allbin':\n",
    "            start_range = 0\n",
    "            end_range = count_Conv2d-1\n",
    "        elif mode == 'nin':\n",
    "            start_range = 1\n",
    "            end_range = count_Conv2d-2\n",
    "        \n",
    "        # Set the range of layers to be binarized\n",
    "        self.bin_range = numpy.linspace(start_range, end_range, end_range-start_range+1).astype('int').tolist()\n",
    "        \n",
    "        # Initialize storage for parameters and target modules\n",
    "        self.num_of_params = len(self.bin_range)\n",
    "        self.saved_params = []\n",
    "        self.target_params = []\n",
    "        self.target_modules = []\n",
    "        index = -1\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                index = index + 1\n",
    "                if index in self.bin_range:\n",
    "                    tmp = m.weight.data.clone()\n",
    "                    self.saved_params.append(tmp)\n",
    "                    self.target_modules.append(m.weight)\n",
    "\n",
    "    def binarization(self, quant_mode='det'):\n",
    "        # Binarization sequence\n",
    "        self.meancenterConvParams()\n",
    "        self.clampConvParams()\n",
    "        self.save_params()\n",
    "        self.binarizeConvParams(quant_mode)\n",
    "\n",
    "    def meancenterConvParams(self):\n",
    "        # Mean Centering of Convolution Parameters\n",
    "        for index in range(self.num_of_params):\n",
    "            s = self.target_modules[index].data.size()\n",
    "            negMean = self.target_modules[index].data.mean(1, keepdim=True).mul(-1).expand_as(self.target_modules[index].data)\n",
    "            self.target_modules[index].data = self.target_modules[index].data.add(negMean)\n",
    "\n",
    "    def clampConvParams(self):\n",
    "        # Clamping Convolution Parameters between -1 and 1\n",
    "        for index in range(self.num_of_params):\n",
    "            self.target_modules[index].data = self.target_modules[index].data.clamp(min=-1.0, max=1.0)\n",
    "\n",
    "    def save_params(self):\n",
    "        # Saving the current parameters\n",
    "        for index in range(self.num_of_params):\n",
    "            self.saved_params[index].copy_(self.target_modules[index].data)\n",
    "\n",
    "    def binarizeConvParams(self, quant_mode):\n",
    "        # Binarize Convolution Parameters\n",
    "        assert quant_mode in ['det', 'sto'], 'No such a quant_mode'\n",
    "        for index in range(self.num_of_params):\n",
    "            n = self.target_modules[index].data[0].nelement()\n",
    "            s = self.target_modules[index].data.size()\n",
    "            m = self.target_modules[index].data.norm(1, 3, keepdim=True).sum(2, keepdim=True).sum(1, keepdim=True).div(n)\n",
    "            if quant_mode == 'det':\n",
    "                self.target_modules[index].data = self.target_modules[index].data.sign().mul(m.expand(s))\n",
    "            elif quant_mode == 'sto':\n",
    "                self.target_modules[index].data = sto_quant(self.target_modules[index].data).mul(m.expand(s))\n",
    "\n",
    "    def restore(self):\n",
    "        # Restore the full precision values back to the weights\n",
    "        for index in range(self.num_of_params):\n",
    "            self.target_modules[index].data.copy_(self.saved_params[index])\n",
    "\n",
    "    def updateBinaryGradWeight(self):\n",
    "        # Update Binary Gradient Weight\n",
    "        for index in range(self.num_of_params):\n",
    "            weight = self.target_modules[index].data\n",
    "            n = weight[0].nelement()\n",
    "            s = weight.size()\n",
    "            m = weight.norm(1, 3, keepdim=True).sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s)\n",
    "            m[weight.lt(-1.0)] = 0\n",
    "            m[weight.gt(1.0)] = 0\n",
    "            m = m.mul(self.target_modules[index].grad.data)\n",
    "            m_add = weight.sign().mul(self.target_modules[index].grad.data)\n",
    "            m_add = m_add.sum(3, keepdim=True).sum(2, keepdim=True).sum(1, keepdim=True).div(n).expand(s)\n",
    "            m_add = m_add.mul(weight.sign())\n",
    "            self.target_modules[index].grad.data = m_add.add(m).mul(1.0-1.0/s[1]).mul(n)\n",
    "\n",
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self, aggregate='mean'):\n",
    "        # Initializer for Weighted Loss\n",
    "        super(WeightedLoss, self).__init__()\n",
    "        assert aggregate in ['normal_ce_mean', 's_ce_mean', 'sc_ce_mean'], 'No such a mode'\n",
    "        self.aggregate = aggregate\n",
    "\n",
    "    def forward(self, input, target, weights=None):\n",
    "        # Compute the loss based on the aggregation mode\n",
    "        if self.aggregate == 'normal_ce_mean':\n",
    "            return F.cross_entropy(input, target)\n",
    "        elif self.aggregate == 's_ce_mean':\n",
    "            sep_loss = F.cross_entropy(input, target, reduce=False)\n",
    "            weights.squeeze_()\n",
    "            assert sep_loss.size() == weights.size(), 'Required size: %r, but got: %r' % (str(sep_loss.size()),str(weights.size()))\n",
    "            return (sep_loss*Variable(weights.cuda().float())).mean()\n",
    "        elif self.aggregate == 'sc_ce_mean':\n",
    "            batch_size = target.data.nelement()\n",
    "            oned_weights = weights[:,target.data.cpu().numpy()].diag()\n",
    "            sep_loss = F.cross_entropy(input, target, reduce=False)\n",
    "            assert sep_loss.size() == oned_weights.size(), 'Required size: %r, but got: %r' % (str(sep_loss.size()),str(oned_weights.size()))\n",
    "            return (sep_loss*Variable(oned_weights.cuda().float())).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks - NIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinActive(torch.autograd.Function):\n",
    "    '''\n",
    "    Binarize the input activations and calculate the mean across channel dimension.\n",
    "    '''\n",
    "    def forward(self, input):\n",
    "        self.save_for_backward(input)\n",
    "        size = input.size()\n",
    "        mean = torch.mean(input.abs(), 1, keepdim=True)\n",
    "        input = input.sign()\n",
    "        return input, mean\n",
    "\n",
    "    def backward(self, grad_output, grad_output_mean):\n",
    "        input, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input.ge(1)] = 0\n",
    "        grad_input[input.le(-1)] = 0\n",
    "        return grad_input\n",
    "\n",
    "class BinConv2d(nn.Module):\n",
    "    '''\n",
    "    Conv layer with vinarized weights and input\n",
    "    '''\n",
    "    def __init__(self, input_channels, output_channels,\n",
    "            kernel_size=-1, stride=-1, padding=-1, dropout=0):\n",
    "        super(BinConv2d, self).__init__()\n",
    "        self.layer_type = 'BinConv2d'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dropout_ratio = dropout\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n",
    "        if dropout!=0:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels,\n",
    "                kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x, mean = BinActive()(x)\n",
    "        if self.dropout_ratio!=0:\n",
    "            x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RealConv2d(nn.Module):\n",
    "    '''\n",
    "    Float conv layer with the same architecture with class::BinConv2d\n",
    "    '''\n",
    "    def __init__(self, input_channels, output_channels,\n",
    "            kernel_size=-1, stride=-1, padding=-1, dropout=0):\n",
    "        super(RealConv2d, self).__init__()\n",
    "        self.layer_type = 'RealConv2d'\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dropout_ratio = dropout\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(input_channels, eps=1e-4, momentum=0.1, affine=True)\n",
    "        if dropout!=0:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels,\n",
    "                kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        #x, mean = BinActive()(x)\n",
    "        if self.dropout_ratio!=0:\n",
    "            x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''\n",
    "    The original binarized XNOR-NIN model\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.xnor = nn.Sequential(\n",
    "                nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #BinConv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(192, 96, kernel_size=1, stride=1, padding=0), # new by simon\n",
    "                #BinConv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d( 96, 192, kernel_size=5, stride=1, padding=2, dropout=0.5),\n",
    "                #BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d(192, 192, kernel_size=3, stride=1, padding=1, dropout=0.5),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.xnor(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n",
    "\n",
    "\n",
    "####################################################\n",
    "#\n",
    "# model variants\n",
    "#\n",
    "####################################################\n",
    "\n",
    "\n",
    "class Net_Cut(nn.Module):\n",
    "    '''\n",
    "    The 'narrower' XNOR-NIN model\n",
    "    '''\n",
    "    def __init__(self, cut_ratio=0.5):\n",
    "        super(Net_Cut, self).__init__()\n",
    "        self.xnor = nn.Sequential(\n",
    "                nn.Conv2d(3, int(192*cut_ratio), kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(int(192*cut_ratio), eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #BinConv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(int(192*cut_ratio), int(96*cut_ratio), kernel_size=1, stride=1, padding=0), # new by simon\n",
    "                #BinConv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d(int(96*cut_ratio), int(192*cut_ratio), kernel_size=5, stride=1, padding=2, dropout=0.5),\n",
    "                #BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=3, stride=1, padding=1, dropout=0.5),\n",
    "                BinConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(int(192*cut_ratio), eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.Conv2d(int(192*cut_ratio),  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.xnor(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class RealNet(nn.Module):\n",
    "    '''\n",
    "    The float NIN model\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(RealNet, self).__init__()\n",
    "        self.xnor = nn.Sequential(\n",
    "                nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #BinConv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                RealConv2d(192, 96, kernel_size=1, stride=1, padding=0), # new by simon\n",
    "                #BinConv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                RealConv2d( 96, 192, kernel_size=5, stride=1, padding=2, dropout=0.5),\n",
    "                #BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                RealConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                RealConv2d(192, 192, kernel_size=3, stride=1, padding=1, dropout=0.5),\n",
    "                RealConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.xnor(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AllBinNet(nn.Module):\n",
    "    '''\n",
    "    binarize the last and first layers of the original XNOR-NIN model\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(AllBinNet, self).__init__()\n",
    "        self.xnor = nn.Sequential(\n",
    "                #nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                #nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                #nn.ReLU(inplace=True),\n",
    "                BinConv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                #BinConv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(192, 96, kernel_size=1, stride=1, padding=0), # new by simon\n",
    "                #BinConv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d( 96, 192, kernel_size=5, stride=1, padding=2, dropout=0.5),\n",
    "                #BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d(192, 192, kernel_size=3, stride=1, padding=1, dropout=0.5),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                #nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                #nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                #nn.ReLU(inplace=True),\n",
    "                BinConv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.xnor(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class NotAllBinNet(nn.Module):\n",
    "    '''\n",
    "    Only binarize the last layer of the original XNOR-NIN model\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(NotAllBinNet, self).__init__()\n",
    "        self.xnor = nn.Sequential(\n",
    "                nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #BinConv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                #BinConv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(192, 96, kernel_size=1, stride=1, padding=0), # new by simon\n",
    "                #BinConv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d( 96, 192, kernel_size=5, stride=1, padding=2, dropout=0.5),\n",
    "                #BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d(192, 192, kernel_size=3, stride=1, padding=1, dropout=0.5),\n",
    "                BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                #nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                #nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                #nn.ReLU(inplace=True),\n",
    "                BinConv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.xnor(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AllBinNet_Cut(nn.Module):\n",
    "    def __init__(self, cut_ratio=0.5):\n",
    "        super(AllBinNet_Cut, self).__init__()\n",
    "        self.xnor = nn.Sequential(\n",
    "                #nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                #nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                #nn.ReLU(inplace=True),\n",
    "                BinConv2d(3, int(192*cut_ratio), kernel_size=5, stride=1, padding=2),\n",
    "                #BinConv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(int(192*cut_ratio), int(96*cut_ratio), kernel_size=1, stride=1, padding=0, dropout=0.0), # new by simon\n",
    "                #BinConv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d( int(96*cut_ratio), int(192*cut_ratio), kernel_size=5, stride=1, padding=2, dropout=0.5),\n",
    "                #BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=1, stride=1, padding=0, dropout=0.0),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=3, stride=1, padding=1, dropout=0.5),\n",
    "                BinConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=1, stride=1, padding=0, dropout=0.0),\n",
    "                #nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                #nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                #nn.ReLU(inplace=True),\n",
    "                BinConv2d(int(192*cut_ratio),  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.xnor(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NotAllBinNet_Cut(nn.Module):\n",
    "    def __init__(self, cut_ratio=0.5):\n",
    "        super(NotAllBinNet_Cut, self).__init__()\n",
    "        self.xnor = nn.Sequential(\n",
    "                nn.Conv2d(3, int(192*cut_ratio), kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(int(192*cut_ratio), eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #BinConv2d(3, int(192*cut_ratio), kernel_size=5, stride=1, padding=2),\n",
    "                #BinConv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(int(192*cut_ratio), int(96*cut_ratio), kernel_size=1, stride=1, padding=0, dropout=0.0), # new by simon\n",
    "                #BinConv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d( int(96*cut_ratio), int(192*cut_ratio), kernel_size=5, stride=1, padding=2, dropout=0.5),\n",
    "                #BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                BinConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=1, stride=1, padding=0, dropout=0.0),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                BinConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=3, stride=1, padding=1, dropout=0.5),\n",
    "                BinConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=1, stride=1, padding=0, dropout=0.0),\n",
    "                #nn.BatchNorm2d(192, eps=1e-4, momentum=0.1, affine=False),\n",
    "                #nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                #nn.ReLU(inplace=True),\n",
    "                BinConv2d(int(192*cut_ratio),  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.xnor(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RealNet_Cut(nn.Module):\n",
    "    def __init__(self, cut_ratio=0.5):\n",
    "        super(RealNet_Cut, self).__init__()\n",
    "        self.xnor = nn.Sequential(\n",
    "                nn.Conv2d(3, int(192*cut_ratio), kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(int(192*cut_ratio), eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                #BinConv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                RealConv2d(int(192*cut_ratio), int(96*cut_ratio), kernel_size=1, stride=1, padding=0), # new by simon\n",
    "                #BinConv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                RealConv2d( int(96*cut_ratio), int(192*cut_ratio), kernel_size=5, stride=1, padding=2, dropout=0.5),\n",
    "                #BinConv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                RealConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=1, stride=1, padding=0),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "\n",
    "                RealConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=3, stride=1, padding=1, dropout=0.5),\n",
    "                RealConv2d(int(192*cut_ratio), int(192*cut_ratio), kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(int(192*cut_ratio), eps=1e-4, momentum=0.1, affine=False),\n",
    "                nn.Conv2d(int(192*cut_ratio),  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.xnor(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BinActive(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Binarize the input activations and calculate the mean across channel dimension.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        size = input.size()\n",
    "        mean = torch.mean(input.abs(), 1, keepdim=True)\n",
    "        input = input.sign()\n",
    "        return input, mean\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, grad_output_mean):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input.ge(1)] = 0\n",
    "        grad_input[input.le(-1)] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class BaseConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dropout=0):\n",
    "        super(BaseConv2d, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_channels, eps=1e-4, momentum=0.1, affine=True)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout else None\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class BinConv2d(BaseConv2d):\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x, mean = BinActive.apply(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = nn.functional.conv2d(\n",
    "            x, self.weight, self.bias, self.stride, self.padding\n",
    "        )\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RealConv2d(BaseConv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(RealConv2d, self).__init__(*args, **kwargs)\n",
    "        self.conv = nn.Conv2d(self.bn.num_features, self.out_channels, self.kernel_size, self.stride, self.padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NetBase(nn.Module):\n",
    "    def __init__(self, cut_ratio=1.0):\n",
    "        super(NetBase, self).__init__()\n",
    "        self.cut_ratio = cut_ratio\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Net(NetBase):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # ... (rest of your initialization logic)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.xnor(x)\n",
    "        x = x.view(x.size(0), 10)\n",
    "        return x\n",
    "\n",
    "# ... (continue with other Net classes, following the same pattern as above)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarized Tensor: \n",
      " tensor([[ 1., -1.,  1.,  1.],\n",
      "        [ 1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1.],\n",
      "        [ 1.,  1., -1.,  1.]])\n",
      "Binarized Filter: \n",
      " tensor([[-1.,  1.],\n",
      "        [ 1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define the sto_quant function\n",
    "def sto_quant(tensor):\n",
    "    return tensor.add(1.).div(2.).add(torch.rand(tensor.size())\\\n",
    "        .add(-0.5)).clamp(0.,1.).round().mul(2.).add(-1.)\n",
    "\n",
    "# Assume tensor is your input tensor and filter_tensor is your filter\n",
    "tensor = torch.tensor([[0.9, -1.2, 0.6, 0.3],\n",
    "                       [-0.4, 2.0, -0.8, 1.0],\n",
    "                       [0.7, -1.5, 1.1, -0.6],\n",
    "                       [0.2, 0.4, -0.9, 0.8]])\n",
    "\n",
    "filter_tensor = torch.tensor([[0.1, -0.2],\n",
    "                              [-0.3, 0.4]])\n",
    "\n",
    "# Applying sto_quant function for binarization\n",
    "binarized_tensor = sto_quant(tensor)\n",
    "binarized_filter = sto_quant(filter_tensor)\n",
    "\n",
    "print(\"Binarized Tensor: \\n\", binarized_tensor)\n",
    "print(\"Binarized Filter: \\n\", binarized_filter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
